{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of 573_LM_RoBERTa.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-lightning transformers "
      ],
      "metadata": {
        "id": "LIh0ubW7HMvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import unicodedata\n",
        "import torch"
      ],
      "metadata": {
        "id": "I2poeghhFpNl"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFBertModel, BertModel, BertConfig,TFBertForSequenceClassification,BertForSequenceClassification, RobertaModel, RobertaForSequenceClassification\n",
        "from transformers import BertTokenizer,RobertaTokenizer\n",
        "\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import (\n",
        "    random_split,\n",
        "    DataLoader,\n",
        "    RandomSampler,\n",
        "    SequentialSampler,\n",
        "    Subset,\n",
        "    TensorDataset\n",
        ")\n",
        "\n",
        "from torch import nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tqdm.notebook import tqdm\n",
        "import os\n",
        "import json\n",
        "import random"
      ],
      "metadata": {
        "id": "guunZm-1PKwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BX27-Y-WFeLS"
      },
      "outputs": [],
      "source": [
        "YelpZip_file_path = '/content/drive/MyDrive/YelpZip'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metadata = pd.read_csv(os.path.join(YelpZip_file_path, 'metadata'), sep= '\\t',\n",
        "                      names=['user_id', 'prod_id','rating','label', 'date'], header=None)\n",
        "with open(os.path.join(YelpZip_file_path, 'reviewContent')) as r:\n",
        "  content = r.readlines()"
      ],
      "metadata": {
        "id": "R32nUwIMFkAq"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(metadata)"
      ],
      "metadata": {
        "id": "yr3JudVKJHNa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f74b65c8-ad4b-4f9e-c9d5-89aa4c64bbca"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "608598"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(content)"
      ],
      "metadata": {
        "id": "reUupVPgJWWD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b8e7a9f-a11a-497c-cba1-d6ae7766c0f1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "608598"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = []\n",
        "label = []\n",
        "# id = {}\n",
        "for text, lab in zip(content, metadata.itertuples(index=True, name='Pandas')):\n",
        "  sentence = text.strip('\\n').split('\\t')[-1]\n",
        "  sentence = unicodedata.normalize(\"NFKD\", sentence)\n",
        "  dataset.append(sentence)\n",
        "  # id[text.user_id] = 1\n",
        "  if lab.label == -1:\n",
        "    l = 0\n",
        "  else: \n",
        "    l = 1\n",
        "  label.append(l)"
      ],
      "metadata": {
        "id": "6RArd8hnFlMZ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset) "
      ],
      "metadata": {
        "id": "XJtwInDiKq5T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80ca71e0-4e07-4f51-a9b7-111ded43444b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "608598"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJvxJPQXmIU2",
        "outputId": "270c638a-c33f-4399-be8d-c61b25f42264"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "608598"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "4J-5UusjO-8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "true"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADpxWCqQbQ7o",
        "outputId": "2c8e1893-f425-4c9f-f161-1af12c7a374d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "527997"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data():\n",
        "    \n",
        "    \n",
        "    x_train, x_test, y_train, y_test = train_test_split(dataset,label, random_state=42,train_size=0.8) #train 80% test 20%\n",
        "\n",
        "    return x_train, x_test, y_train, y_test\n",
        "\n",
        "x_train, x_test, y_train, y_test = load_data()"
      ],
      "metadata": {
        "id": "mwA6LYQ4FmtC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")"
      ],
      "metadata": {
        "id": "rgA-JL93P8yG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_data_train = tokenizer.batch_encode_plus(\n",
        "    x_train,\n",
        "    add_special_tokens=True, \n",
        "    return_attention_mask=True, \n",
        "    pad_to_max_length=True,\n",
        "    # is_split_into_words = True, \n",
        "    max_length=128, \n",
        "    return_tensors='pt'\n",
        ")\n",
        "\n",
        "\n",
        "encoded_data_test = tokenizer.batch_encode_plus(\n",
        "    x_test, \n",
        "    add_special_tokens=True, \n",
        "    return_attention_mask=True, \n",
        "    pad_to_max_length=True, \n",
        "    max_length=128, \n",
        "    return_tensors='pt'\n",
        ")\n",
        "\n",
        "input_ids_train = encoded_data_train['input_ids']\n",
        "attention_masks_train = encoded_data_train['attention_mask']\n",
        "labels_train = torch.tensor(y_train)\n",
        "\n",
        "input_ids_test = encoded_data_test['input_ids']\n",
        "attention_masks_test = encoded_data_test['attention_mask']\n",
        "labels_test = torch.tensor(y_test)\n",
        "\n",
        "dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
        "dataset_test = TensorDataset(input_ids_test, attention_masks_test, labels_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LyVkrgZzP3Bn",
        "outputId": "b2c86a52-9ec7-4547-e026-2c4072021ac5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQh-86b4FUJS",
        "outputId": "9c0a1eb3-f27d-415a-c835-07c7153d5909"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataset.TensorDataset at 0x7fd4a7aac950>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\",\n",
        "                                                      # problem_type=\"multi_label_classification\",\n",
        "                                                      num_labels=2,\n",
        "                                                      output_attentions=False,\n",
        "                                                      output_hidden_states=False)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QE0hqbx2FdTY",
        "outputId": "db198ece-4247-4706-f581-87f7c232d98a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.bias', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "batch_size = 32\n",
        "epochs = 3\n",
        "learning_rate = 5e-5\n"
      ],
      "metadata": {
        "id": "tOz6zYwlFjvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dataloader_train = DataLoader(dataset_train, \n",
        "                              sampler=RandomSampler(dataset_train), \n",
        "                              batch_size=batch_size)\n",
        "\n",
        "dataloader_test = DataLoader(dataset_test, \n",
        "                                   sampler=SequentialSampler(dataset_test), \n",
        "                                   batch_size=batch_size)"
      ],
      "metadata": {
        "id": "9x_lS5YzFPuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr=learning_rate, \n",
        "                  eps=1e-8)\n",
        "                  \n",
        "\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps=0,\n",
        "                                            num_training_steps=len(dataloader_train)*epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRDjOCYmFvKA",
        "outputId": "f5234e5a-8f9e-482c-fd24-af9b307ee4b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_accuracy(pred, labels):\n",
        "  preds_flat = np.argmax(pred, axis=1).flatten()\n",
        "  labels_flat = labels.flatten()\n",
        "  \n",
        "  return accuracy_score(labels_flat, preds_flat)\n",
        "\n",
        "# def get_binary_accuracy(pred, labels):\n",
        "#   preds_flat = torch.round(torch.sigmiod(pred).flatten())\n",
        "#   labels_flat = labels.flatten()\n",
        "  \n",
        "#   return accuracy_score(labels_flat, preds_flat)"
      ],
      "metadata": {
        "id": "8bHMpIPtGCy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "seed_val = 17\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\")\n",
        "\n",
        "def evaluate(dataloader_val):\n",
        "\n",
        "    model.eval()\n",
        "    \n",
        "    loss_val_total = 0\n",
        "    predictions, true_vals = [], []\n",
        "    \n",
        "    for batch in dataloader_val:\n",
        "        \n",
        "        batch = tuple(b.to(device) for b in batch)\n",
        "        \n",
        "        inputs = {'input_ids':      batch[0],\n",
        "                  'attention_mask': batch[1],\n",
        "                  'labels':         batch[2],\n",
        "                 }\n",
        "\n",
        "        with torch.no_grad():        \n",
        "            outputs = model(**inputs)\n",
        "            \n",
        "        loss = outputs[0]\n",
        "        logits = outputs[1]\n",
        "        loss_val_total += loss.item()\n",
        "\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = inputs['labels'].cpu().numpy()\n",
        "        predictions.append(logits)\n",
        "        true_vals.append(label_ids)\n",
        "    \n",
        "    loss_val_avg = loss_val_total/len(dataloader_val) \n",
        "    \n",
        "    predictions = np.concatenate(predictions, axis=0)\n",
        "    true_vals = np.concatenate(true_vals, axis=0)\n",
        "            \n",
        "    return loss_val_avg, predictions, true_vals\n",
        "    \n"
      ],
      "metadata": {
        "id": "6kSmDSM9F2EQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wVUM6I465BfU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "CUDA_LAUNCH_BLOCKING=1\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "for epoch in tqdm(range(1, epochs+1)):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    loss_train_total = 0\n",
        "\n",
        "    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n",
        "    for batch in progress_bar:\n",
        "\n",
        "        model.zero_grad()\n",
        "     \n",
        "        batch = tuple(b.to(device) for b in batch)\n",
        "        \n",
        "        inputs = {'input_ids':      batch[0],\n",
        "                  'attention_mask': batch[1],\n",
        "                  'labels':         batch[2],\n",
        "                 }       \n",
        "\n",
        "        outputs = model(**inputs)\n",
        "        \n",
        "        loss = outputs[0]\n",
        "        loss_train_total += loss.item()\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        \n",
        "        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
        "         \n",
        "        \n",
        "    # torch.save(model.state_dict(), f'/finetuned_BERT_epoch_{epoch}.model')\n",
        "        \n",
        "    tqdm.write(f'\\nEpoch {epoch}')\n",
        "    \n",
        "    loss_train_avg = loss_train_total/len(dataloader_train)            \n",
        "    tqdm.write(f'Training loss: {loss_train_avg}')\n",
        "    \n",
        "    val_loss, predictions, true_vals = evaluate(dataloader_test)\n",
        "    accuracy = get_accuracy(predictions, true_vals)\n",
        "    tqdm.write(f'Testing loss: {val_loss}')\n",
        "    tqdm.write(f'Accuracy: {accuracy}')\n",
        "\n",
        "torch.save(model.state_dict(), f'/content/drive/MyDrive/RoBERTa_base_573.model')"
      ],
      "metadata": {
        "id": "uelCpxf2F5Mo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}